<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Data Quest]]></title>
  <link href="http://vivekmishra1991.github.io/atom.xml" rel="self"/>
  <link href="http://vivekmishra1991.github.io/"/>
  <updated>2015-10-14T20:18:01+01:00</updated>
  <id>http://vivekmishra1991.github.io/</id>
  <author>
    <name><![CDATA[Vivek Mishra]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Random Forest :And How its better than bagging]]></title>
    <link href="http://vivekmishra1991.github.io/blog/2015/10/09/random-forest-feature-selection-metric/"/>
    <updated>2015-10-09T15:53:58+01:00</updated>
    <id>http://vivekmishra1991.github.io/blog/2015/10/09/random-forest-feature-selection-metric</id>
    <content type="html"><![CDATA[<h3>Randomforests And Bootstrap Aggregating</h3>

<p>As we know in simple averaging methods RFs perform better than Bootstrap Aggregating or <em>bagging</em> by considerably reducing the high variance (see <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias-variance tradeoff</a>) of a single classifier by continuous independent sampling with replacement from the same distribution(dataset). This performance gain of RFs is described very well in <a href="http://machinelearning202.pbworks.com/w/file/fetch/60606349/breiman_randomforests.pdf">this</a> Leo Breiman(2001) paper and also in <a href="http://www-stat.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning : Hastie,Tibshirani,Friedman </a> in the section RandomForests.</p>

<p>I will briefly describe the idea of performance gain in Rfs over bagging here.Since in both the ensembling methods a single classifier(decision trees) ,which is very prone to high variance, when makes use of all the available features,it tends to over-fit since a more complex tree dependence structure is generated. Averaging over different classifiers, with low collinearity among them, decreases the over-all complexity of the final ensembled model. But including all the feature is not a good idea,since all the features might not help much in overall information gain for individual classifier and many a time can prove to be a source of noise.To overcome this problem <strong>L. Breiman</strong> proposed the idea of selecting features randomly from some fixed number of features..</p>

<!-- more -->


<h3>Code</h3>

<p>In the following code snippet, oob error is calculated for each estimator that is being added to the classifier grown on <em>Boston_housing_data</em> and a graph is plotted on matplotlib for the same.</p>

<p><strong>Note</strong> : I have used sqrt(Number of features) as <em>max_feature</em> which is the recommended number of features to be sampled from the distribution.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>from collections import OrderedDict
</span><span class='line'>from sklearn.datasets import load_boston
</span><span class='line'>import numpy as np
</span><span class='line'>from sklearn.ensemble import RandomForestClassifier
</span><span class='line'>from sklearn.ensemble import BaggingClassifier
</span><span class='line'>from sklearn.datasets import make_classification
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>
</span><span class='line'>RAND=123
</span><span class='line'>
</span><span class='line'># Generate a binary classification dataset.
</span><span class='line'>boston = load_boston()
</span><span class='line'>X= boston.data
</span><span class='line'>#y= boston.target
</span><span class='line'>y= np.array(boston.target).astype(str)
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>ensemble_clfs=[("RandomForestClassifier",RandomForestClassifier(warm_start=True, oob_score=True,
</span><span class='line'>                               max_features="sqrt",
</span><span class='line'>                               random_state=RAND)), 
</span><span class='line'>               ("BaggingClassifier", BaggingClassifier(
</span><span class='line'>                              oob_score=True,
</span><span class='line'>                              random_state=RAND) 
</span><span class='line'>               )
</span><span class='line'>               
</span><span class='line'>             ]
</span><span class='line'>
</span><span class='line'>#initialize error rate dictionary 
</span><span class='line'>error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)
</span><span class='line'>
</span><span class='line'>#Required Params
</span><span class='line'>max_est = 100
</span><span class='line'>min_est = 10
</span><span class='line'>
</span><span class='line'>for label,clf in ensemble_clfs:
</span><span class='line'>    for i in range( min_est, max_est + 1 ):
</span><span class='line'>        clf.set_params( n_estimators = i )
</span><span class='line'>        clf.fit( X, y )
</span><span class='line'>        #Out_of_bag Score(OOB score)
</span><span class='line'>        oob_err=1-clf.oob_score_
</span><span class='line'>        error_rate[label].append((i,oob_err))
</span><span class='line'>
</span><span class='line'>#print oob_err_list vs ensemble classifier
</span><span class='line'>for label,ensemble_err in error_rate.items():
</span><span class='line'>        Xn_est,Yoob_err=zip(*ensemble_err)
</span><span class='line'>        plt.plot(Xn_est,Yoob_err,label=label)    
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>plt.xlim(min_est, max_est)
</span><span class='line'>plt.xlabel("n_estimators")
</span><span class='line'>plt.ylabel("OOB error rate")
</span><span class='line'>plt.legend(loc="upper right")
</span><span class='line'>plt.show()
</span></code></pre></td></tr></table></div></figure>


<h3>OOb_Score Vs n_estimators Plot</h3>

<p><img src="http://vivekmishra1991.github.io/./assets/rf_bagging/img.png" alt="Figure1" /></p>

<p>In the above plot we tried to plot <strong>out-of-bag error</strong>(error estimated over currently ensembled classifiers with data point not in dataset or out of the bag) on Y-axis and X-axis shows <em>&ldquo;n_estimator&rdquo;</em> which describes the number of estimator ensembled till now . As the number of estimator increase overall variance reduces for both the ensembling methods. But Random Forest is the clear winner all the way while growing the forest where #Tress equals 100.
This shows how random feature selection generalizes the final model and reduces over-fitting and variance than choosing all the features.</p>

<p>Also remember,the performance of RFs is very much dependent upon the collinearity between the  estimators being grown for the forests.Lesser the collinearity ,better is the performance of the estimator.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AdaBoost : Why it is robust to overfitting]]></title>
    <link href="http://vivekmishra1991.github.io/blog/2015/09/28/adaboost-why-it-is-robust-to-overfitting/"/>
    <updated>2015-09-28T16:49:29+01:00</updated>
    <id>http://vivekmishra1991.github.io/blog/2015/09/28/adaboost-why-it-is-robust-to-overfitting</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<h4>Bagging vs. Boosting</h4>

<p>There are two reasons for errors in an estimator namely variance or bias.A too complex model(<em>unpruned decision trees</em>) have high variance but low bias  whereas a too simple model( <em>Weak learners like decision stumps</em> ) have high bias but low variance.To minimize these two types of errors two different approaches are required namely <em>bagging</em>(complex models/high variance) and <em>boosting</em>(simple models/high bias). </br>
In more general terms bagging also known as <em>bootstrap aggregating</em> tries to build complex models over many bootstrapped samples from the same distribution(datasets) and gives simpler but more robust model either by simple averaging or majority voting over former complex models.Whereas boosting is an approach of ensembling strong learners from weak learners simply by improving over the misclassified labels from dataset in a finite number of iterations.In this blog we will try to shed some light over the learning approach of boosting meta-algorithm .</p>

<!-- more -->


<h2>AdaBoost</h2>

<h4>Weak Learners And Boosting</h4>

<p>In 1988 Michael Kearns remarked whether one can boost a weak learner to a strong learner which was answered by a <a href="http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf">remarkable paper</a> by Rob Schapire published in 1990. Thereby paving a way for a novel approach of utilizing the power of a weak learners two give a better , versatile and more robust strong learners.<strong>Yoav Freund</strong> and <strong>Robert Schapire</strong> came together  to work on a much strong &amp; refined meta-algorithm called AdaBoost or <em>Adaptive Boosting</em> which won them the prestigious Gödel Prize in 2003.But before we dive into realms of Adaboost algorithm we need to have a clear understanding of what a weak learner is.</br>
    By weak I mean that classifier does a better job at classifying than randomly guessing but not by much i.e. it classifies  slightly better than 50% of the correct labels in the sampled outputs.A weak classifer could be a simple decision stump(<em>A stump is a two-node tree, after a single split.</em>), C4.5 and ID3 generated decision trees  also perform well as weak learners for adaboost.</p>

<h4>Algorithm</h4>

<p><img src="http://vivekmishra1991.github.io/./assets/adaboost/algo.jpg" alt="Adaboost Algorithm" /></p>

<ol>
<li>If there are N datum in the dataset then initially we distribute equal weights to all datapoints (wi=1/N).</br></li>
<li>We take <strong>M</strong> iterations from <strong>m=1 to M</strong> and for each <em>m</em></br>
a) Fit a weak classifier to the training data using above weights.</br>
b) <em>err</em> is calculated as weighted average of mis-classified dataum </br>
c) <em>alpha</em> for each iteration is calculated. <em>err</em> must be less then 0.5 ,i.e. weak learners should be better than random guesses, for alpha>1.<br>
d) exp(Indicator function(I)) gives 1 when I=0 hence weight remain unchanged else when I=1 weight is readjusted(increases) to <em>weight*alpha</em>.</br></li>
<li>Now new classifier is weighted sum of on each M iterations' alpha value and classifier output.</li>
</ol>


<p><img src="http://vivekmishra1991.github.io/./assets/adaboost/schematic.jpg" alt="schematic representation" /><em> <strong>Fig-2</strong> Schematic representation of AdaBoost; with the dataset on the left side,the different widths of the bars represent weights applied to each instance. The weighted predictions pass through a classifier, which is then weighted by the triangles (alpha values). The weighted output of each triangle is summed up in the circle, which produces the final output.</em>
<strong>Source</strong>:MLIA-Peter Harrington</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FrugalKmeans: Restaurants' home delivery route optimizer]]></title>
    <link href="http://vivekmishra1991.github.io/blog/2015/09/19/frugalkmeans-restaurants-home-delivery-route-optimizer/"/>
    <updated>2015-09-19T11:41:55+01:00</updated>
    <id>http://vivekmishra1991.github.io/blog/2015/09/19/frugalkmeans-restaurants-home-delivery-route-optimizer</id>
    <content type="html"><![CDATA[<h2>Introduction</h2>

<p>What if I could reduce the home delivery cost of restaurants around my city.Maybe optimize routes , cluster restaurants into similar regions and assign delivery men to each cluster. Would this help ? Lets find out.</p>

<p>FrugalKmeans tries to analyze this over my favorite city <em>Lucknow</em>,India which is known for its exquisite <em>Awadhi</em> Cuisine .</p>

<p>After-all <strong>FRUGALITY IS OPTIMAL</strong>.</p>

<!-- more -->


<h2>DATA</h2>

<p>I required <strong>names</strong> and <strong>addresses</strong> of all the restaurants in Lucknow. My only option was scraping data from my favorite food portal. <a href="http://scrapy.org/">Scrapy</a> an open source scraping tools came very handy in getting the names of all the restaurants.There were whooping <em>(for non-metro standard) 962</em> of them in Lucknow. <a href="https://github.com/vivekmishra1991/FrugalKMeans/blob/master/scrap/spiders/frugalMeans.py">Github-Scrap </a> has the required scripts &amp; instruction for doing this.</p>

<p>Sample scraped data:(<strong>TABLE 1</strong>)</p>

<table>
<thead>
<tr>
<th> <strong>address</strong>                                      </th>
<th> <strong>name</strong>                         </th>
</tr>
</thead>
<tbody>
<tr>
<td>                                                  </td>
<td>                              </td>
</tr>
<tr>
<td> _____________________________________________________</td>
<td>________________________</td>
</tr>
<tr>
<td> 41/47, Opposite Kendriya Vidyalaya, Vivek Khand  </td>
<td>       180 Chocoxpress              </td>
</tr>
<tr>
<td> 3rd Floor, Hera Plaza, Sector 8, Vikas Nagar, </td>
<td>          101 Temperature Lounge       </td>
</tr>
<tr>
<td> Capital Building, GPO, Hazratganj, Lucknow       </td>
<td>       A 1 Dildar Sindhi Restaurant </td>
</tr>
<tr>
<td> 4th Floor, Food Court, Phoenix United Mall, Alambagh </td>
<td>   180 Chocoxpress              </td>
</tr>
<tr>
<td> 5th Floor, Phoenix United Mall, Alambagh, Lucknow </td>
<td>       5 C R Lounge                 </td>
</tr>
</tbody>
</table>


<h3>Coordinates</h3>

<p>After obtaining the names and addresses of all the restraunt ,next step was to get their coordinates.<a href="https://developers.google.com/maps/documentation/geocoding/intro">Google-Geo-coding API</a> does the trick.Just get a <code>API KEY</code> as instructed in aforementioned link and using <a href="https://github.com/vivekmishra1991/FrugalKMeans/blob/master/frugalKmeans/co_ordinates/coordinate.py">coordinate.py</a> you will get a nice list of restaurants with their coordinates.</p>

<p>Something like this:(<strong>Table 2</strong>)</p>

<table>
<thead>
<tr>
<th> <strong>name</strong>                                    </th>
<th> <strong>address</strong>                                                                                                        </th>
<th> <strong>Lat.</strong>        </th>
<th> <strong>Long.</strong>       </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td> __________________________________________</td>
<td>___________________________________________</td>
<td>______________________</td>
<td>___________</td>
</tr>
<tr>
<td>                                                                                                                </td>
<td>            </td>
<td>            </td>
<td></td>
</tr>
<tr>
<td> Hera Plaza Sector 8 Vikas Nagar lucknow </td>
<td> Sector 8, Vikas Nagar, Lucknow, Uttar Pradesh 226022, India                                                    </td>
<td> 26.8906639  </td>
<td> 80.9670749 </td>
</tr>
<tr>
<td> Kamla Nehru Marg lucknow                </td>
<td> Kamla Nehru Marg, Lucknow, Uttar Pradesh 226003, India                                                         </td>
<td> 26.8658443 </td>
<td> 80.9104684 </td>
</tr>
<tr>
<td> GPO Hazratganj Lucknow lucknow          </td>
<td> Sardar Patel Park, Vidhan Sabha Marg, Raj Bhavan Colony, The Mall Avenue, Lucknow, Uttar Pradesh 226001, India </td>
<td> 26.8453894 </td>
<td> 80.9456421 </td>
</tr>
</tbody>
</table>


<h2>DATA PREPROCESSING</h2>

<p>Now we have all the data that we require to analyze our problem.But still a little trimming is required to make the available data suitable to be fed into our algorithm.</p>

<p>First step would be to using <a href="http://pandas.pydata.org/">Pandas</a> library to weed out the addresses &amp; names and also fill up the NA data point if any. After data processing stage we are left with coordinates only.</p>

<p>Something like:(<strong>Table 3</strong>)</p>

<table>
<thead>
<tr>
<th> <strong>Latitude</strong>   </th>
<th> <strong>Longitude</strong>  </th>
</tr>
</thead>
<tbody>
<tr>
<td>___________________</td>
<td>___________</td>
</tr>
<tr>
<td> 26.8466937 </td>
<td> 80.946166  </td>
</tr>
<tr>
<td> 26.8466937 </td>
<td> 80.946166  </td>
</tr>
<tr>
<td> 26.8466937 </td>
<td> 80.946166  </td>
</tr>
<tr>
<td> 26.8906639 </td>
<td> 80.9670749 </td>
</tr>
<tr>
<td> 26.8658443 </td>
<td> 80.9104684 </td>
</tr>
</tbody>
</table>


<h2>Clustering</h2>

<p>Next step in this quest to find a frugal method for home delivery comes with the clustering of restaurants into similar geographical regions.And what is a better method then K-Means to achieve that.But there is a problem with K-Means for this dataset and it fails giving NA values as the cluster centroids, if <code>K&gt;4</code>.</p>

<p>This is a serious issue pretaining to K-Means therefore I somehow settled for <strong>&lsquo;Bi-KMeans&rsquo;</strong> or <strong>Bisecting KMeans</strong> clustering algorithm which is a little different form the traditional K-means algorithm in following ways.</p>

<p>k-means sometimes gives poor results if it is caught in a local minimum.Therfore
another algorithms called bisecting k-means is developed. It starts with choosing one random centroid and distance from all the points is calculated as sum of square error or <strong>SSE</strong>. While number of centroids are less then <strong>K</strong>(user defined number of clusters) algorithms recursively splits the above cluster into many clusters based on the least SSE error for the split.</p>

<p>After aplying Bisecting K-means clustering algorithm(<a href="https://github.com/vivekmishra1991/FrugalKMeans/blob/master/frugalKmeans/biKMeans.py">Github-BiKmeans.py</a>) to the data in Table3 we get another list of data points with two features &ldquo;<strong>the cluster they belong to</strong>&rdquo; and &ldquo;<strong>distance from corresponding centroid</strong>&rdquo; . A sample data from after transformation :</p>

<table>
<thead>
<tr>
<th> Cluster </th>
<th>  SSE or distance </th>
</tr>
</thead>
<tbody>
<tr>
<td>_________</td>
<td>__________________ </td>
</tr>
<tr>
<td> 8       </td>
<td> 0.5886538506     </td>
</tr>
<tr>
<td> 3       </td>
<td> 0.0241409154     </td>
</tr>
<tr>
<td> 3       </td>
<td> 0.0107502713     </td>
</tr>
<tr>
<td> 3       </td>
<td> 0.0107502713     </td>
</tr>
<tr>
<td> 7       </td>
<td> 0.2401804062     </td>
</tr>
<tr>
<td> 0       </td>
<td> 0.1848409829     </td>
</tr>
</tbody>
</table>


<h2>Results and Visualizations</h2>

<p><a href="Generate%20custom%20maps%20using%20your%20coordinates%20in%20Excel!">Hamstermap</a> is a beautiful online tool for plotting map coordinates but since it accepts a specific format we need to do a little Data munging for that. A python script(<a href="https://github.com/vivekmishra1991/FrugalKMeans/blob/master/frugalKmeans/frugalKmeans.py">frugalKmeans.py</a>) I have written does the same very efficiently .After transforming the data with above script we are left with the following list of <a href="https://github.com/vivekmishra1991/FrugalKMeans/tree/master/frugalKmeans/hamstermap">data points</a> :</p>

<table>
<thead>
<tr>
<th> latitude   </th>
<th> longitude  </th>
<th> marker  </th>
<th> color  </th>
</tr>
</thead>
<tbody>
<tr>
<td>_________________</td>
<td>____________</td>
<td>_________</td>
<td>________</td>
</tr>
<tr>
<td> 26.8466937      </td>
<td> 80.946166  </td>
<td> circle1 </td>
<td> blue   </td>
</tr>
<tr>
<td> 26.8466937      </td>
<td> 80.946166  </td>
<td> circle1 </td>
<td> blue   </td>
</tr>
<tr>
<td> 26.8906639      </td>
<td> 80.9670749 </td>
<td> circle1 </td>
<td> blue   </td>
</tr>
<tr>
<td> 26.8658443      </td>
<td> 80.9104684 </td>
<td> circle1 </td>
<td> orange </td>
</tr>
<tr>
<td> 26.8453894      </td>
<td> 80.9456421 </td>
<td> circle1 </td>
<td> blue   </td>
</tr>
<tr>
<td> 26.8466937      </td>
<td> 80.946166  </td>
<td> circle1 </td>
<td> blue   </td>
</tr>
<tr>
<td> 26.8466937      </td>
<td> 80.946166  </td>
<td> circle1 </td>
<td> blue   </td>
</tr>
<tr>
<td> 26.8532449      </td>
<td> 80.9965217 </td>
<td> circle1 </td>
<td> black  </td>
</tr>
</tbody>
</table>


<p>Exporting this to Hamstermap and plotting the required coordinates gives the centroids(where delivery boys should be stationed) and respective restaurants they would service from.</p>

<p>After some exploratory data-analysis we come up with following map.<img src="http://vivekmishra1991.github.io/./assets/result.jpg" alt="Optimal Points to cover entire Lucknow delivery space" /></p>
]]></content>
  </entry>
  
</feed>
